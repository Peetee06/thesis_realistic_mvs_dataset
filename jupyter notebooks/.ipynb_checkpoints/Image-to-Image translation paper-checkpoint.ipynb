{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image-to-Image Translation with Conditional Adversarial Networks\n",
    "*by Isola et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Defintion: image-to-image translation is the task of translating one possible representation of a scene into another (representations are e.g. RGB-image, gradient field (?), semantic label map,...)  \n",
    "-> Predicting pixels from pixels  \n",
    "CNNs commonly used for this (minimize a loss function while learning)  \n",
    "Problem: naive approach Euclidean distance between predicted and ground truth pixels will produce blurry results  \n",
    "finding a loss function that outputs sharp, realistic images is an open problem, generally requires expert knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Goal: specify high-level goal, e.g. \"make the output indistinguishable from reality\", automatically learn a loss function appropriate for satisfying this goal.\n",
    "  \n",
    "-> GANs (Generate Adversarial Networks)  \n",
    "learn loss that tries to classify if output image is real or fake, simultaneously training a generative model to minimize this loss  \n",
    "GANs learn a loss that adapts to data, so no need to manually design loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "code at https://github.com/phillipi/pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using conditional GANs (cGANs) that learn a conditional generative model  \n",
    "  \n",
    "The framework of the authors is not application-specific and therefore considerably simpler than most other prior and concurrent works.  \n",
    "Use a \"U-Net\"-based architecture for the generator and convolutional \"PatchGAN\" classifier for the discriminator which only penalizes structure at the scale of image patches  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method\n",
    "\n",
    "GANs are generative models  \n",
    "learn mapping from random noise vector $z$ to output image $y$, $G: z \\rightarrow y$.  \n",
    "conditional GANs learn mapping from observed image x and random noise vector z to y, $G: \\{x,z\\} \\rightarrow y$.  \n",
    "Generator G is trained to produce images that can't be distinguished from real images by an adversarially trained discriminator D, which is trained to do as well as possible at detecting the generator's \"fakes\".\n",
    "\n",
    "![](./images/I2I_Figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective\n",
    "\n",
    "general objective of conditional GAN:\n",
    "$$\n",
    "\\mathcal{L}_{cGAN}(G,D)= E_{x,y}[\\log D(x,y)]+E_{x,z}[\\log (1-D(x,G(x,z))]\n",
    "$$  \n",
    "and  \n",
    "$$\n",
    "G^* = \\arg  \\min_G \\max_D \\mathcal{L}_{cGAN}(G,D)\n",
    "$$  \n",
    "  \n",
    "Comparison to unconditional variant:  \n",
    "$$\n",
    "\\mathcal{L}_{GAN}(G,D) = E_y[\\log D(y)]+ E_{x,z}[\\log(1-D(G(x,z))]\n",
    "$$  \n",
    "  \n",
    "Previous approaches have found it beneficial to mix the GAN objective with more traditional loss, such as L2 distance. Authors use L1 distance as it encourages less blurring than L2\n",
    "$$\n",
    "\\mathcal{L}_{L1}(G) = E_{x,y,z}[\\Vert y-G(x,z)\\Vert_1]\n",
    "$$  \n",
    "resulting in the final objective  \n",
    "$$\n",
    "G^* = \\arg \\underset{G}{\\min}\\underset{D}{\\max} \\mathcal{L}_{cGAN}(G,D) + \\lambda \\mathcal{L}_{L1}(G)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems\n",
    "Without z, the net could still learn a mapping from x to y, but would produce deterministic outputs, and therefore fail to match any distribution other than a delta function. Therefore past cGANs provided Gaussian noise z as input to the generator, in addition to x. \n",
    "In the work of the authors the generator learned to ignore the noise though.  \n",
    "Therefore they provided noise only in the form of dropout applied on several layers of the generator at both training and test time. Despite that the authors observe only minor stochasticity in the output of their nets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Network architecture\n",
    "\n",
    "Both generator and discriminator use modules of the form convolution-BatchNorm-ReLu (see 44 and 29 in References)  \n",
    "Details are in supplemental materials online  \n",
    "Also use encoder-decoder network with skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markovian discriminator (PatchGAN)\n",
    "\n",
    "L2 and L1 loss functions produce blurry results on image generation problems.  \n",
    "They work fine on low frequencies, but fail to enforce correctness at high frequencies  \n",
    "  \n",
    "Autors therefore use discriminator to only model high-frequency structure and use L1 term to force low-frequency correctness.  \n",
    "For high-frequencies a \"PatchGAN\" is used as discriminator architecture, which tries to classify if each $N \\times N$ patch in the image is real or fake.  \n",
    "run convolutionally across the image, averaging responses for the ultimate output of $D$.  \n",
    "Section 4.4 shows that this approach still produces high quality results compared to classifying the full image size instead of patches.  \n",
    "Advantageous because fewer parameters, runs faster, can be applied to arbitrarily large images  \n",
    "Effectively models image as a Markov random field, assuming independence between pixels separated by more than a patch diameter.  \n",
    "This PatchGAN can be understood as a form of texture/style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization and Inference\n",
    "\n",
    "alternate between one gradient descent step on D, then one on G  \n",
    "train G to maximize $\\log D(x, G(x,z))$  \n",
    "divide objective by 2 while optimizing D, which slows down the rate at which D learns relative to G  \n",
    "Use minibatch SGD (stochastic gradient descent)\n",
    "apply Adam solver with learning rate of 0.0002 and momentum parameters $\\beta_1 = 0.5$, $\\beta_2 = 0.999$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To explore generality of cGANs, authors test on variety of tasks and datasets  \n",
    "e.g. day $\\rightarrow$ night, sketch $\\rightarrow$ photo, Edges $\\rightarrow$ photo (more details see paper section 4)  \n",
    "Even on small datasets often decent results  (e.g. facade training set 400 images, day to night training 91 images)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Reference 52 about difficulty of evaluating quality of synthesized images\n",
    "Evaluation metrics in Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation metrices\n",
    "\n",
    "first: run \"real vs. fake\" perceptual studies on Amazon Mechanical Turk (AMT)  \n",
    "second: measure whether or not the synthesized cityscapes are realistic enough that off-the-shelf recognition systems can recognize the objects in them  \n",
    "-> FCN score: classification accuracy comparing real vs fake images\n",
    "\n",
    "When comparing different patch sizes (1x1, 16x16, 70x70 and full image 286x286) by their FCN scores, the 70x70 patchGAN gets the highest scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other methods yield better results on colorization and semantic segmentation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
