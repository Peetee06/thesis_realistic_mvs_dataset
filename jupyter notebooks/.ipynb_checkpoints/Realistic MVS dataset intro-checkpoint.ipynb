{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of papers\n",
    "\n",
    "by Peter Trost  \n",
    "Rottenburg (Germany), 10/2018  \n",
    "peter_trost93@yahoo.de  \n",
    "or peter.trost@student.uni-tuebingen.de (until ~08/2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "convert slides: jupyter nbconvert *.ipynb* --to slides --post serve\n",
    "\n",
    "multi view stereo (MVS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Naturalistic Open Source Movie for Optical Flow Evaluation  \n",
    "  \n",
    "*by Butler et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "- dataset for optical flow evaluation derived from 3D animated short film *Sintel*\n",
    "- contains long sequences, large motions, specular reflections, motion blur, defocus blur, atmospheric effects and more\n",
    "- scenes of open source graphics data rendered in varying complexity\n",
    "- can be used to improve optical flow methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Render passes\n",
    "\n",
    "- Albedo Pass: Flat, unshaded, surfaces exhibit constant albedo over time\n",
    "- Clean Pass: Illumination including smooth shading and specular reflections adds realism\n",
    "- Final Pass: Full rendering with all effects including blur due to camera depth of field and motion, and atmospheric effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Main aspects\n",
    "\n",
    "*and comparing to Middlebury flow benchmark*\n",
    "\n",
    "- Difficulty: Sintel dataset contains varying and more challenging (for existing methods) scenes\n",
    "- Sequence Length: 50 frames (2 to 8 in Middlebury) with 49 ground truth flow fields (only for one pair per sequence in Middlebury)\n",
    "- Amount of Data: 1628 frames of ground truth flow (100 times Middlebury), with 564 for test and 1064 for training (good for machine learning)\n",
    "- Image Resolution: 1024 x 436 px (45% to 100% more than Middlebury frames)\n",
    "- Large Motions: well over 100 pixels per frame (Middlebury max 35 ppf)\n",
    "- Blur: Middlebury doesn't contain motion or defocus blur, Sintel contains renders with and without them\n",
    "- Motion Boundaries and Occluded Regions: new definition of motion boundaries, new error measure (function of distance from boundaries)\n",
    "- Real-World Challenges: lighting variations, shadows, complex materials and reflections and more contained in Sintel\n",
    "- Transparency: no transparency included\n",
    "- Ranking: ranked according to average endpoint error (EPE) in different challenge categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Blender's internal motion blur pipeline was modified to give accurate motion vectors at each pixel which provide ground truth optical flow maps\n",
    "- clips were selected so that optical flow is realistic. one still has to be cautious when training and evaluating algorithms that strongly rely on real-world laws of physics\n",
    "- images saved as 8-bit PNG files and framerate of 24 fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "The MPI-Sintel Flow Data Set is publicly available at http://sintel.is.tue.mpg.de and\n",
    "includes:\n",
    "- image sequences for Albedo, Clean and Final passes.\n",
    "- for training set: forward flow fields (floating point and color image visualizations), occlusion boundary masks, unmatched pixel masks and invalid pixel maps\n",
    "- software to compute various error statistics on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image statistics\n",
    "\n",
    "Sintel dataset is compared to datasets of lookalike clips with natural scenes\n",
    "\n",
    "- images are converted to grey-scale $I(x,y) \\in [0,255]$ to compare luminance statistics. Kullback-Leibler-Divergence from Sintel to Lookalikes is 0.058 (smaller than Middlebury to Lookalikes, being 0.176)\n",
    "- (?) Spatial power spectra (p. 10 last paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analysis\n",
    "\n",
    "computed flow for the Sintel dataset on following publicly available methods:\n",
    "- Classic+NL (good modern method)\n",
    "- Classic++ (standard robust method)\n",
    "- LDOF (deisgned to deal with large displacement optical flow)\n",
    "- Horn and Schunck (HS) (classic method)\n",
    "- Anisotropic Huber-L1 Flow (GPU-based optical flow method)\n",
    "\n",
    "Results:\n",
    "average endpoint error (EPE) for these methods on Middlebury is $\\leq$ 0.5. For Sintel it's between 7.43 and 12.67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./images/Sintel_analysis_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Playing for Data: Ground Truth from Computer Games  \n",
    "  \n",
    "*by Richter et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "- uses detouring (inject wrapper between game and OS) to record, modify and reproduce rendering commands from Grand Theft Auto 5 (GTA5)\n",
    "- hashing distinct rendering resources (geometry, textures, shaders) to create object signatures and therefore pixel-accurate object labels which enables propagating these labels across time and instances that share distinctive resources\n",
    "- contains 25.000 images from GTA5 with pixel-level semantic segmentation ground-truth\n",
    "- labeling took 49 hours (3 orders of magnitude faster than other semantic segmentation datasets with similar annotation density)\n",
    "- through propagation of object labels across images annotation time per image decreases sharply during annotation process of multiple images (stays constant in labeling interfaces prior to this approach -> linear increase with size of dataset)\n",
    "- uses labeling compatible with other datasets for urban scene understanding\n",
    "- models trained with game data and $\\frac{1}{3}$ of CamVid dataset outperform models trained with full CamVid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting information from the rendering pipeline\n",
    "\n",
    "- Games communicate with the hardware through APIs (e.g. OpenGL, Direct3D, Vulkan)\n",
    "- Autors implemented a wrapper for DirectX 9 API and used RenderDoc for wrapping Direct3D 11\n",
    "- enables them to monitor creation, modification and deletion of resources used to specify the scene and synthesize an image\n",
    "- recorded every 40th frame during GTA5 gameplay\n",
    "- RenderDoc was modified to record data in a format suitable for annotation\n",
    "- data collected during gameplay session is processed in batch afterwards (30 seconds per frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Challenges:\n",
    "- identifying relevant function calls:  \n",
    "group calls by the G-buffers that were assigned as render targets\n",
    "- identifying resources:  \n",
    "hash (128-bit key, non-cryptographic) memory content describing a mesh to recognize same meshes between different gaming sessions. Volatile resource IDs are then mapped to persistent hash keys\n",
    "- Formatting annotation:  \n",
    "use two render passes. The first is the conventional renderpass, the second is used to encode IDs into pixels so that each pixel contains resource IDs for mesh, texture and shader of the scene element imaged at that pixel. Four render targets with three 8-bit color channels are used resulting in 96 bits per pixel containing the three 32-bit resource IDs. These are mapped to the 128-bit hash keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Semantic Labeling\n",
    "\n",
    "- patch decomposition:  \n",
    "images are decomposed into patches of pixels that share the same mesh, texture and shader (MTS). Objects generally consist of multiple patches, patches usually are contained within a single object, are associated with underlying surfaces in the scene and are linked to other patches that depict same surface. patch boundaries coincide with semantic class boundaries. \n",
    "Grouping patches creates pixel-accurate label maps\n",
    "![](./images/PlayingForData_patching_illustration.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- association rule mining:\n",
    "uses statistical regularities in associations between resources and semantic labels to label other patches that use this resource aswell (e.g. car meshes are highly likely to only be used on cars)\n",
    "- annotation process:\n",
    "uses annotation tool. annotator clicks on button of a specific semantic class then on the to be labeled patch in the image. The annotation tool automatically propagates the annotation to other images if above requirements are met. Annotation time per image decreases during the process (tool presents only images that have more than 3% of their area unlabeled). Also small unrecognizable areas are likely propagated back when later images containing that area are labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### dataset and analysis\n",
    "- image resolution 1914 x 1052\n",
    "- 49 hours to label 98.3% of pixel area with corresponding classes\n",
    "- per image labeling time 2-3 magnitudes faster than KITTI and CamVid datasets\n",
    "- 98.7% of images more than 90% are pre-annotated by the time they are reached by annotator\n",
    "- high variability: median of 4 images containing same MTS. 26.5% of MTS combinations occur in only one image each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Semantic Segmentation\n",
    "\n",
    "- training: first stage on real and synthetic data using mini-batch stochastic gradient descent with mini-batches of 8 images (4 real, 4 synthetic). 50k iterations, learning rate $10^{-4}$, momentum 0.99. Cropsize 628x628, receptive field 373x373 pixels. second stage fine-tuning for 4k iterations, real data only, same parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SyB3R: A Realistic Synthetic Benchmark for 3D Reconstruction from Images  \n",
    "  \n",
    "*by Ley et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "source no. 8 provides dataset containing architectural objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### overview\n",
    "\n",
    "- framework\n",
    "- uses realistically rendered images\n",
    "- Blender with path tracing (to simulate more complex light-surface interactions)\n",
    "- camera parameters and 3D structure of the scene are known\n",
    "- provides:  \n",
    "  - automatic synthesis framework with full control about the scene and the image formation  \n",
    "  - several datasets with a variety of challenging characteristics  \n",
    "  - results of a few example experiments illustrating the benefits of synthetic but realistic benchmarks  \n",
    "  - flexible and open-source C++ implementation of the proposed framework (http://andreas-ley.com/projects/SyB3R/)\n",
    "- image generation is split into rendering the scene into 2D image and post-processing that implements remaining effects in image space\n",
    "- all steps implemented in modular manner (allows exchanging, reordering and turning modules off/on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image Rendering\n",
    "- uses \"Cycles\" in Blender (Monte-Carlo path tracer, accurate propagation of light through scene)\n",
    "- produced images are stored as HDR (retains full floating-point precision of all intensity values)\n",
    "- Cycles handles: scene properties (lighting, surface texture, ...), object motion, large camera motion, camera properties (focal length, principal point, resolution, depth of field (DoF), field of view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Image post-processing\n",
    "  \n",
    "chain of individual modules:\n",
    "![chain of individual modules](./images/SyB3R_postProcessingChain.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes  \n",
    "  \n",
    "*by Ros et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "- synthetic dataset of urban scenes\n",
    "- generated to aid semantic segmentation in context of autonomous driving\n",
    "- consists of photo-realistic frames\n",
    "- pixel-level semantic annotations for 13 classes\n",
    "- frames acquired from multiple view-points\n",
    "- frames contain associated depth-map\n",
    "- generated by rendering virtual city created with Unity development platform\n",
    "- includes four different seasons with drastic change of appearance (lighting, weather conditions,...)\n",
    "- variety of illumination conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SYNTHIA-Rand\n",
    "- consists of 13,400 frames of the city\n",
    "- from camera randomly moving around city\n",
    "\n",
    "### SYNTHIA-Seq\n",
    "- four videosequences\n",
    "- approx. 50,000 frames each\n",
    "- one per season\n",
    "- simulating a car moving through city (includes interaction with objects, slowing down, speeding up)\n",
    "- omnidirectional view on demand (cameras in all 4 directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Results\n",
    "- DCNNs trained solely on SYNTHIA-Rand frames (resized to 180x120 pixels) are almost as or more accurate than trained on real data\n",
    "- DCNNs trained on batches of 6 images of real and 4 of synthetic domain have better per-class accuracy than those trained solely on real data (up to 18.3 percentage points more)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
