{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n",
    "    by Jun-Yan Zhu et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/Unpaired_I2I_Figure1.PNG)\n",
    "\n",
    "Style transfers from paintings to photos, photos to different paintingstyles, horses to zebras, from on season to another. And vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/Unpaired_I2I_Figure2.PNG)\n",
    "\n",
    "Other approaches used paired data (left side) to learn mapping. Paired training data is expensive and often not available. This approach's goal is to learn a mapping $G : X \\rightarrow Y$ where the distribution of images $G(X)$ is as close as possible to that of Y using an adversarial loss. As we have unpaired data this wouldn't yield good enough results so the author's also used an inverse mapping $F : Y \\rightarrow X$ and introduced a cycle consistency loss to enforce $F(G(X)) \\approx X$ and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "method learns to find special characteristics of one image collection and how these could be translated into the other image collection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/Unpaired_I2I_Figure3.PNG)\n",
    "Model includes mappings $G: X \\rightarrow Y$, $F: Y \\rightarrow X$ and adversarial discriminators $D_X$ and $D_Y$ where $D_X$ tries to distinguish between $x$ and $F(y)$ and $D_Y$ between $y$ and $G(x)$  \n",
    "  \n",
    "- adversarial loss for matching distribution of generated images to data distribution of target domain\n",
    "- cycle consistency loss to prevent learned mappings G and F from contradicting each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adversarial loss\n",
    "$$\\mathcal{L}_{\\text{GAN}}(G,D_Y, X, Y) = E_{y\\sim p_{\\text{data}}(y)}[\\log(D_Y(y))] + E_{x\\sim p_{\\text{data}}(x)}[\\log(1-D_Y(G(x))]$$  \n",
    "With \n",
    "$$\\min_G \\max_{D_Y} \\mathcal{L}_{\\text{GAN}}(G, D_Y, X, Y)$$\n",
    "### Cycle Consistency Loss\n",
    "*forward cycle consistency*:  \n",
    "$x \\rightarrow G(x) \\rightarrow F(G(x)) \\approx x$  \n",
    "*backward cycle consistency*:  \n",
    "$y \\rightarrow F(y) \\rightarrow G(F(y)) \\approx y$  \n",
    "This behavior is forced by *cycle consistency loss*:  \n",
    "$$ \\mathcal{L}_{\\text{cyc}}(G,F) = E_{x\\sim p_{\\text{data}}(x)}[\\Vert F(G(x)) - x\\Vert_1] + E_{y\\sim p_{\\text{data}}(y)}[\\Vert G(F(y)) - y \\Vert_1]$$\n",
    "\n",
    "### Full Objective\n",
    "$$\\mathcal{L}(G,F, D_X, D_Y)=\\mathcal{L}_{\\text{GAN}}(G,D_Y, X, Y) + \\mathcal{L}_{\\text{GAN}}(F,D_X, Y, X) + \\lambda \\mathcal{L}_{\\text{cyc}}(G,F)$$  \n",
    "  \n",
    "$$ G^*, F^* = \\arg \\min_{G,F} \\max_{D_X, D_Y} \\mathcal{L}(G,F,D_X,D_Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
