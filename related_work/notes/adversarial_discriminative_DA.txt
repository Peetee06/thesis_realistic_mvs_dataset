Kontradiktorische Lernmethoden sind ein vielversprechender Ansatz um robuste, tiefe Netze  zu trainieren und können komplexe Proben in  diversen Domänen generieren. Sie können auch Erkennung trotz vorhandenseins einer Domänenverschiebung oder Datensatzvoreinnahmen verbessern: neueste kontradiktorische Herangehensweisen an unüberwachte Domänenanpassung reduzieren den Unterschied zwischen der Trainings- und Testdomänenverteilung und verbessern dadurch die Verallgemeinerungsleistung. Allerdings, während Generative kontradiktorische Netzwerke (GANs) ansehnliche Visualisierungen zeigen, sind sie nicht optimal bei Unterscheidungsaufgaben  und können auf kleinere Verschiebungen limitiert sein. Andererseits können unterscheidende Herangehensweisen mit größeren Domänenverschiebungen umgehen, zwingen dem Modell aber gebundene Gewichtungen auf und nutzen keinen GAN-basierten Verlust aus. In dieser Arbeit umreißen wir zuerst einen neuen, generalisierten Rahmen für kontradiktorische Anpassung, die topaktuelle Herangehensweisen als Spezialfälle zusammenfasst und verwenden diese verallgemeinerte Sicht, um bisherige Herangehensweisen besser in Relation zu stellen.  
