\chapter{Experiments}
\label{sec:experiments}

In this chapter the three Domain Adaptation Techniques \textit{Cycle Consistent Adversarial Domain Adaptation}, \textit{Cycle-consistent Generative Adversarial Network} and \todo{add 3rd technique} will be compared by analysing similarities and differences. Furthermore pretrained models of each architecture were tested and the results will be compared on their Intersection over Union scores \todo{add benchmark(s)}.

%\section{training the nets on tcml cluster} \todo{is this necessary or can we just use provided pre-trained models?}
%To train the models the tcml cluster of uni t√ºbingen was used (\todo{add link to cluster website}). The cluster contains a lot of computing power with multiple compute nodes and a storage node. Each compute node has 4 CPUs and a NVidia 1080 Ti GPU and lots of RAM. In order to run the training code, it is necessary to create an .sbatch file. This file specifies how long a script needs to run, how much memory it needs and includes bash commands to run the script. Depending on what is specified in that .sbatch file the slurm manager system allocates ressources for that job and runs it as soon there are resources ready. While training cycleGAN mode collapse happened and the test images didn't get translated at all. Also to train for the 200 recommended epochs it would take around 2-3 weeks due to training taking a day for around 14 epochs. This is probably due to the fact that training cycleGAN is only possible on batches of size 1 which makes the vast resources available on the cluster not usable to their full potential. Also the student account can only run one job at once which makes it impossible to train different methods (cycleGAN, CyCADA, GradGAN) at the same time. Another issue is that it was not possible to visualize loss of generators and discriminators in order to supervise the training process and check if mode collapse or any other issue appeared. 

\section{Datasets}

\subsection{Synthetic dataset: \\
	Playing for Data: Ground Truth from Computer Games}
see \cite{Richter_2016_ECCV}

The GTA5 (Grand Theft Auto V) dataset is proposed in \cite{Richter_2016_ECCV}. It contains 24966 images taken from a street view in the game Grand Theft Auto V by Rockstar Games \cite{GTAV}. The images are provided with $1914 \times 1052$ pixels and are containing moving cars, objects, pedestrians, bikes, have changing lighting and weather conditions aswell as day and night scenes. For all of these images the authors provide label images that are compatible with those of the Cityscapes dataset \cite{Cordts_2016_CVPR}. Detouring, i.e. injecting a wrapper between the game and the graphics hardware to log function calls and reconstruct the 3D scene is used. This enables a faster labeling process as objects in a scene can be assigned an object ID through which the assigned label is propagated to other images containing this same object. Due to being more realistic than other existing synthetic street view datasets (e.g. SYNTHIA \cite{RosCVPR16}) the GTAV dataset is very popular for training machine learning models related to autonomous driving and is therefore used in this work.


\subsection{Real dataset:\\
	The Cityscapes Dataset for Semantic Urban Scene Understanding}
see \cite{Cordts_2016_CVPR}

The Cityscapes dataset is a large scale dataset containing dashcam view images from 50 european cities. It includes 30 classes relevant for autonomous driving. The images include scenes in spring, summer and fall seasons and under different weather conditions. There are 5000 images provided together with fine annotations and 20000 together with coarse annotations. Due to the large amount of labeled data from a dashvam view and the inclusion of scenes with different weather and lighting conditions this dataset is often used to train deep neural networks that are related to autonomous driving. Due to the popularity and the GTAV dataset containing compatible labeling maps, this work uses Cityscapes as the real dataset for the experiments.

\section{comparison Benchmark(s)}
\subsection{Intersection over Union (IoU)}
The Intersection over Union is a metric often used to compare semantic segmentation methods (\todo{add references to works that use it}). It follows following formula:
\begin{align*}
	\frac{\text{predicted Pixels} \cap \text{ground truth Pixels}}{\text{predicted Pixels} \cup \text{ground truth Pixels}}
\end{align*}
where predicted Pixels are the pixels predicted for a specific class by the semantic segmentation model and ground truth Pixels are the Pixels containing the ground truth for that image. Usually there are multiple different classes to predict and therefore it is common to calculate the mean IoU (mIoU) over all images that have predictions. To compare how well classes themselves are predicted by a model, one can also calculate the class IoU (cIoU).

\subsection{Perceptual Loss}
Leave this out due to time?
