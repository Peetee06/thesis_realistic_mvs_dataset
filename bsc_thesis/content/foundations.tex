\chapter{Foundations}
\label{sec:foundations}

This chapter will describe and explain the foundations necessary for this work. The term \textbf{Domain Adaptation} will be defined and described. Furthermore all relevant \textbf{Neural Network} architectures will be shown and explained. Specifically \textbf{Convolutional Neural Networks} and \textbf{Generative Adversarial Networks}.


\section{Domain Adaptation}
As described in \cite{DBLP:journals/corr/Csurka17},
Domain Adaptation is the task of transfering a machine learning model that is working well on a source data distribution to a related target data distribution. In this work we will focus on the adaptation from synthetic to real images. When talking about a synthetic image we are implying it was rendered from a virtual scene through a rendering engine like for example blender's ''Cycles`` \cite{Cycles} or graphics engine like Unity \cite{Unity}. We define real images as taken from a real-world scene through some kind of camera. In general there are many more domains, for example an image of a painting in the style of a particular artist, images of an object from different viewpoints, and many more.

\begin{figure}
	\centering
	 \includegraphics[width=0.8\textwidth]{../images/DA_examples_cityscapes_gta.png}
	\caption{Example images for the the two domains relevant for this work. A real image from the cityscapes dataset \cite{Cordts_2016_CVPR} (left) and a synthetic image from the GTA5 dataset \cite{Richter_2016_ECCV} (right)}
\end{figure}

\section{Neural Networks}

\subsection{Convolutional Neural Networks}
see \cite{wu2017introduction} (Introduction to CNNs)
Convolutional Neural Networks (CNNs) are Deep Neural Networks mostly used with images as input, consisting of convolutional layers, that extract features from input data (e.g. edges, curves, circles), pooling layers, commonly using max-pooling (mapping a subregion of the input to its maxium value) or average pooling (mapping the subregion to the average of the values) and a fully connected network often used for classification. 
\todo{add more description and example images}

\paragraph{Convolutional Layer.}
In this layer a so called kernel iterates over the input. The kernel is a matrix of fixed size depending on the method used. A 3x3 kernel is commonly used. These kernels are used to extract features like diagonal lines, horizontal lines, curves and more. For example to extract a diagonal line a kernel could look like this:
$$
\begin{pmatrix}
	1 & 0 & 0\\
	0 & 1 & 0\\
	0 & 0 & 1
\end{pmatrix}
$$

\paragraph{Pooling Layer}
This layer reduces the size of the image by mapping an area to its maximum or average value in order to focus on the important features and save computing time.


\paragraph{Fully Connected Layer}
The last type of layers is a standard feed forward network that predicts has number of possible classes as output nodes. It is used to classify the input given the features exracted by the preceeding layers. 


\todo{example architectures VGG16, AlexNet,..?}


\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) implement a two-player-game:\\
A Discriminator learns from a given data distribution what is ``real''. The Generator generates data. The goal of the generator is to fool the discriminator into believing the generated data is ``real'' i.e. tries to create samples coming from the same distribution as the ``real'' data. The discriminator will label anything as ``fake'' that doesn't resemble the learned ``real'' data distribution (2 classes classification, real or fake). This way GANs can learn to generate realistically looking images of faces, translate images of art from one style to another and improve semantic segmentation.
The generative model generally uses \textit{maximum likelihood estimation}. In \cite{DBLP:journals/corr/Goodfellow17} it is described in the following way:
\begin{quote}
	The basic idea of maximum likelihood is to define a model that provides an estimate of probability distribituion, parametereized by parameters $\theta$. We then refer to the \textbf{likelihood} as the probability that the model assigns to the training data: $\prod_{i=1}^{m}p_{\text{model}}(x^{(i)}; \theta)$
\end{quote}
The parameter $\theta$ that maximizes the likelihood of the data is better found in $\log$ space \cite{DBLP:journals/corr/Goodfellow17}
\begin{align}
	\theta^* &= \underset{\theta}{\arg \max} \prod_{i = 1}^{m} p_{\text{model}} (x^{(i)}; \theta)\\
	&= \underset{\theta}{\arg \max} \log \prod_{i=1}^{m} p_{\text{model}}(x^{(i)}; \theta)\\
	&= \underset{\theta}{\arg \max} \sum_{i = 1}^{m} \log p_{\text{model}}(x^{(i)}; \theta)
\end{align}
as the maximum of the function is at the same $\theta$ value and we now have a sum which aswell eliminates the possibility of having underflow by multiplying multiple very small probabilities together.
Formally GANs are a structured probabilistic model (more info: Chapter 16 of Goodfellow et al. 2016) containing latent variables z and observed variables x.


\paragraph{Generators}
defined by a function G that takes $\mathbf{z}$ as input and uses $\theta^{(G)}$ as parameters

\paragraph{Discriminators}
defined by a function D that takes $\mathbf{x}$ as input and uses $\theta^{(D)}$ as parameters

Both players have cost functions that are defined in terms of both players' parameters. The discriminator  wishes to minimize $J^{(D)}(\theta^{(D)}, \theta^{(G)})$ and must do so by controlling only $\theta^{(D)}$. This is analogous for the generator: he tries to minimize $J^{(G)}(\theta^{(D)}, \theta^{(G)})$ while controlling only $\theta^{(G)}$. In contrast to an optimization problem that has a solution that is the (local) minimum (a point in parameter space where all neighboring points have greater or equal cost), the GAN objective is a game. The solution to a game is a Nash equilibrium (\todo{reference john forbes nash jr.}), meaning that each player chooses the best possible option or strategy in respect to what the other player(s) choose. Here the Nash equilibrium is a tuple $(\theta^{(D)}, \theta^{(G)})$ that is a local minimum of $J^{(D)}$ with respect to $\theta^{(D)}$ and a local minimum $J^(G)$ with respect to $\theta^{(G)}$.
\paragraph{Training}

\paragraph{Advantages and Disandvantages}