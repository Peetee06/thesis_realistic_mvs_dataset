\chapter{Domain Adaptation techniques}
\label{sec:techniques}

This chapter will give an overview over the three \textbf{Domain Adaptation techniques} that will be compared in this work, namely \textbf{CycleGAN} \cite{DBLP:journals/corr/ZhuPIE17}, \textbf{CyCADA} \cite{DBLP:journals/corr/abs-1711-03213} and \textbf{SG-GAN} \cite{DBLP:journals/corr/abs-1801-01726}. It will explain how these techniques work, what kind of Network Architecture they use and how training these networks was approached. See table \ref{table:DA_comparison} for an overview of what losses the techniques use.

All of the methods compared in this work are based on Generative Adversarial Networks which were initially proposed in \cite{NIPS2014_5423} and are all designed to be able to train on unpaired data which is necessary for the synthetic-to-real task focused on in this work. See chapter \ref{sec:foundations} for more context and details.

\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\multicolumn{1}{c}{} & \multicolumn{3}{c}{Methods}\\
		\cline{2-4}
		\multicolumn{1}{c|}{} & CycleGAN & CyCADA & SG-GAN\\
		\hline
		cycle-consistency loss & $\checkmark$ & $\checkmark$ & $\checkmark$\\
		\hline
		semantic loss & - & $\checkmark$ & $(\checkmark)$\\
		\hline 
		soft-gradient loss & - & - & $\checkmark$\\
		\hline
	\end{tabular} 
	\caption{Comparison of loss functions of Domain Adaptation techniques \textbf{CycleGAN} \cite{DBLP:journals/corr/ZhuPIE17}, \textbf{CyCADA} \cite{DBLP:journals/corr/abs-1711-03213} and \textbf{SG-GAN} \cite{DBLP:journals/corr/abs-1801-01726}}
	\label{table:DA_comparison}
\end{table}

\section{CycleGAN} 

\subsection{Loss Functions}
As proposed in \cite{DBLP:journals/corr/ZhuPIE17}, CycleGAN adds a cycle-consistency loss which consists of an additional generator/discriminator pair with the idea that translating a previously translated image back to the source domain should yield the original image again. Formally: let $G: X \rightarrow Y$ be the mapping function of the Generator translating an image from the source $X$ to the target domain $Y$. Now add mapping function $F: Y \rightarrow X$ for the second generator's translation from target back to the source domain. The goal of cycle-consistency is that for any given image $x\in X$ from the source domain, $F(G(x)) \approx x$ holds. This analogously also has to hold for any image $y \in Y$ of the target domain with $G(F(y)) \approx y$. As described in chapter \ref{sec:foundations}, GANs implement a two player game. 
The loss function of this game for CycleGAN is described as
\begin{align}
\mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y\sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x\sim p_{\text{data}}(x)}[1 - \log D_Y(G(x))]
\end{align}
with $D_Y$ being the discriminator that classifies samples $y \in Y$ as real or fake. This loss function is analogous for generator $F$ and discriminator $D_X$. The cycle-consistency loss is defined as
\begin{align}
\mathcal{L}_{\text{cyc}}(G, F) =  \mathbb{E}_{x\sim p_{\text{data}}(x)} [\lVert F(G(x))- x \rVert_1] + \mathbb{E}_{y\sim p_{\text{data}}(y)} [\lVert G(F(y))- y \rVert_1] \label{eq:GAN_loss}
\end{align}
with $\lVert \cdot \rVert_1$ being the L1-norm, which is the sum of absolute deviations between $x$ and $F(G(x))$ and $y$ and $G(F(y))$. The full objective for the CycleGAN method is therefore:
\begin{align}
\mathcal{L}(G,F,D_X,D_Y) = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F) \label{eq:CycleGAN_loss}
\end{align}

with $\lambda$ controlling how much focus lies on the cycle-consistency. The discriminators' goal is to label the samples generated by the generators as fake while the generator tries to generate samples that the discriminators will label as real. This results in the goal of discriminators to maximize Equation \ref{eq:CycleGAN_loss} and generators to minimize it. This leads to the goal for CycleGAN: 
\begin{align}
G^*, F^* = \arg \underset{G,F}{\min}\underset{D_X, D_Y}{\max} \mathcal{L}(G,F,D_X,D_Y) \label{eq:GAN_goal}
\end{align}

\subsection{Implementation}
\subsubsection{Network Architecture}
The authors adopt the network architecture proposed in \cite{DBLP:journals/corr/JohnsonAL16}. The network contains two stride-2 convolutions, several residual blocks as described in \cite{DBLP:journals/corr/HeZRS15}, and two fractionally-strided convolutions with stride $\frac{1}{2}$. The authors use 6 blocks for $128 \times 128$ images and 9 blocks for $256 \times 256$ and higher resolution training images. Instance normalization as well as a Discriminator with $70 \times 70$ PatchGANs is used. The latter aims to classify patches of $70\times 70$ pixels of the images as real or fake. The advantage of this PatchGAN is that it has fewer parameters than full-image Discriminators and can be used on images of arbitrary size in a fully convolutional way.

\subsubsection{Training Details}
To stabilize the model training procedure the authors replaced the negative log likelihood objective by a least-squares loss as it is more stable during training and yields higher quality results. Formally train generator $G$ to minimize 
\begin{align*}
	\mathbb{E}_{x\sim p_{\text{data}}(x)}[(D(G(x)) - 1)^2]
\end{align*} 
and discriminator $D$ to minimize
\begin{align*}
	\mathbb{E}_{y\sim p_{\text{data}}(y)}[(D(y) - 1)^2] + \mathbb{E}_{x\sim p_{\text{data}}(x)}[D(G(x))^2]
\end{align*}
By updating the discriminator using a history of 50 generated images model oscillation is reduced. They set $\lambda = 10$ in Equation \ref{eq:CycleGAN_loss}, use the Adam solver with a batch size of 1 and train the networks from scratch using a learning rate of 0.0002 while linearily reducing it to zero between the 100th and the 200th epoch.

\section{CyCADA} 
\subsection{Loss Functions}
CyCADA \cite{DBLP:journals/corr/abs-1711-03213} adds a semantic loss to the CycleGAN approach to enforce semantic consistency (no more translating cats to dogs or drawing trees into the sky). This is achieved by doing semantic segmentation on the source image, then translating that image and doing semantic segmentation on the target image. The loss describes the difference between the two resulting label maps. Formally they begin by learning a source model $f_X$ that performs semantic segmentation on the source data. For K-way classification with cross-entropy loss this results in the following loss for the classification with $L_X$ being the source labels:
\begin{align}
	\mathcal{L}_{\text{task}}(f_X, X, L_X) = -\mathbb{E}_{(x, l_X)\sim(X, L_X)}\sum_{k=1}^{K}\mathds{1}_{[k=l_X]}\log\Bigl(\sigma(f_X^{(k)}(x))\Bigr) \label{eq:task_loss} 
\end{align}
where $\sigma$ denotes the softmax function.\\
With equation \ref{eq:task_loss} the semantic loss is defined as:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{sem}}(G, F, X, Y, f_X) &= \mathcal{L}_{\text{task}}(f_X, F(Y), p(f_X, Y))\\
		&+ \mathcal{L}_{\text{task}}(f_X, G(X), p(f_X, X))
	\end{split}
\end{align}
with $p(f_X, x)$ being the label predicted by source semantic segmentation model $f_X$ on a sample $x \in X$. Together with Equation \ref{eq:CycleGAN_loss} this results in the full objective of the CyCADA approach:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{CyCADA}}&(f_Y, X, Y, L_X, G, F, D_X, D_Y)\\
		&= \mathcal{L}_{\text{task}}(f_Y, G(X), L_X)\\
		&+ \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X)\\
		&+ \mathcal{L}_{\text{cyc}}(G, F, X, Y) + \mathcal{L}_{\text{sem}}(G, F, X, Y, f_X)
	\end{split}
\end{align}
And ultimately optimizing for a target model $f_Y$ with:
\begin{align}
	f^*_Y = \underset{f_Y}{\arg\min} ~ \underset{F,G}{\min} ~ \underset{D_X, D_Y}{\max} ~ \mathcal{L}_{\text{CyCADA}}&(f_X, X, Y, L_X, G, F, D_X, D_Y)
\end{align}

\subsection{Implementation}
The authors use the task loss on labeled source data to pre-train the source task model $f_X$ that performs semantic segmentation on the source images. Then they combine their image space GAN losses with semantic segmentation consistency and cycle consistency losses to perform pixel-level adaptation of the images. From this the parameters for the translations $G$ and $F$ and the discriminators $D_X$ and $D_Y$ are obtained. Additionaly an initial version of the the task model $f_Y$ trained on translated source images and their corresponding source labels is learned. To conclude training the authors learn a feature discriminator $D_{\text{feat}}$ which guides the representation update of $f_Y$. This is done in order to adapt the feature space of $f_Y$ to obtain features that are aligned between translated source images and real target images. The generator and discriminator losses are weighted equally for all feature space adaptation. If the discriminator's accuracy exceeds $60\%$ over the last 100 iterations the generator is updated to reduce volatile training. 

\subsubsection{Network Architecture}
The authors use VGG16-FCN8s \cite{DBLP:journals/corr/LongSD14} as well as DRN-26 \cite{DBLP:journals/corr/YuKF17} during their experiments. 

\subsubsection{Training Details}
Using the FCN8s architecture the authors train their source semantic segmentation model for 100k iterations using SGD, a learning rate of 0.001 and momentum of 0.9. For DRN-26 they use SGD as well with the same hyper-parameters but for 115k iterations. Both times they crop the images to $600 \times 600$ and train on batches of 8 images. They used the original hyper-parameters and network architecture of CycleGAN \cite{DBLP:journals/corr/ZhuPIE17} for cycle-consistent image adaptation. The images were scaled to 1024 pixels width and randomly cropped to patches of $400 \times 400$ for training. They were trained for 20 epochs. The authors used SGD with momentum 0.99 and learning rate 0.00001 for feature level adaptation training. %TODO check last few sentences and D_feat again if time


\section{SG-GAN}

\subsection{Loss Functions}
SG-GAN \cite{DBLP:journals/corr/abs-1801-01726} further expands the above-mentioned losses by including a gradient-sensitive objective that emphasizes semantic boundaries by regularizing the generator to render distinct color/texture for each semantic region, and a patch-level discriminator that better understands differences in appearance distributions of different semantic regions (e.g. coarse texture of asphalt vs smooth and reflective of a vehicle). The soft gradient-sensitive objective is achieved by convolving gradient filters each upon an image and its semantic labeling. Typically Sobel filters are used:
\begin{align}
	\begin{array}{cc}
		C_x = 
		\begin{pmatrix}
			-1 & 0 & 1\\
			-2 & 0 & 2\\
			-1 & 0 & 1
		\end{pmatrix},
		C_y = 
		\begin{pmatrix}
			1 & 2 & 1\\
			0 & 0 & 0\\
			-1 & -2 & -1
		\end{pmatrix}
	\end{array}
	\label{eq:sobel}
\end{align}
These filters extract vertical (y-direction) and horizontal (x-direction) edges in an image. To get the gradient $C$ at each position, the following formula is used:
\begin{align}
	C = \sqrt{C_x^2 + C_y^2}
\end{align}
With input image $v$ and corresponding semantic labeling $s_v$, the gradient sensitive loss can be defined as follows:
\begin{align}
		l_{\text{grad}}(v,s_v,G_{V\rightarrow R}) &= \lVert(|(|C_i * v |-|C_i*G_{V\rightarrow R}(v)|)|) \odot sgn(C_s*s_v)\rVert_1
\end{align}
where $C_i$ is the gradient filter for the image, $C_s$ the one for its semantic labeling map and $G_{V\rightarrow R}$ is the generator that maps from \underline{V}irtual to \underline{R}eal domain. $*$ describes the convolution of the filters over the image, $\odot$ is the element-wise multiplication, $|\cdot|$ is the absolute value- and $sgn$ the sign function. The loss grows, the higher the difference between gradients of image $v$ and image $G_{V\rightarrow R}(v)$. The element-wise multiplication with $sgn(C_s * s_v)$ enforces that the loss focuses on semantic boundaries only. For the soft gradient loss it may be believed that $v$ and $G_{V\rightarrow R}(v)$ share similar texture within semantic classes and therefore one will have edges where the other does as well. The following formula is defined for the soft gradient-sensitive loss in which $\beta$ controls how much belief is set into the texture similarities:
\begin{align}
	\begin{split}
		l_{s-\text{grad}}(v, s_v, G_{V\rightarrow R}, \alpha, \beta) &= \lVert(|(|C_i*v|-|C_i*G_{V\rightarrow R}(v)|)|)\\
		&\odot (\alpha \times |sgn(C_s*s_v)|+\beta)\rVert_1\\
		&s.t. \quad \alpha + \beta = 1 \quad \alpha, \beta \geq 0
	\end{split} 
	\label{eq:soft-grad}
\end{align}
With Equation \ref{eq:soft-grad} defining the full soft gradient-sensitive loss with generator $G_{R\rightarrow V}$ from real to virtual and semantic labeling for real images $R$ as $S_R$ and $S_V$ for virtual images $V$:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta) &= \mathbb{E}_{r\sim p_{\text{data}}(r)}[l_{s-\text{grad}}(r,s_r, G_{R\rightarrow V}, \alpha, \beta)]\\
		&+ \mathbb{E}_{v\sim p_{\text{data}}(v)}[l_{s-\text{grad}}(v,s_v, G_{V\rightarrow R}, \alpha, \beta)]
	\end{split}
\end{align}
The final objective with $\lambda_c, ~ \lambda_g$ controlling importance of cycle consistency- and soft gradient-sensitive loss relative to adversarial loss and semantic-aware Discriminators $SD_R,~SD_V$ is:
\begin{align}
	\begin{split}
		\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R) &= \mathcal{L}_{\text{GAN}}(G_{V\rightarrow R}, SD_R, V, R)\\
		&+ \mathcal{L}_{\text{GAN}}(G_{R\rightarrow V}, SD_V, R, V)\\
		&+ \lambda_c \mathcal{L}_{\text{cyc}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R)\\
		&+ \lambda_g \mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta)
	\end{split}
	\label{eq:final_obj_SG-GAN}
\end{align}
Which results in the optimization target:
\begin{align}
	G^*_{V\rightarrow R}, G^*_{R\rightarrow V} &= \arg \underset{\substack{G_{V\rightarrow R}\\ G_{R\rightarrow V}}}{\min}~ \underset{\substack{SD_R\\SD_V}}{\max}\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R)
\end{align}

\subsection{Implementation}
The afore-mentioned semantic-aware discriminator enforces higher-level semantic consistencies. For example, compared to the virtual world, the real world may have less illumination. Instead of darkening the tone of the whole image it is instead more accurate to keep the sky bright and only darken e.g. the road. To achieve this semantic-aware judgement of realism, the discriminators' last layers' number of filters is transited to the number of semantic classes. Semantic masks are then applied upon these filters in order to make them focus on different semantic classes. Usually, the last layer's feature map of the discriminator is a tensor $\mathbf{T}$ with shape $(w,h,1)$ with $w$ being the width and $h$ being the height. The SG-GAN approach changes this to $(w,h,s)$ where $s$ is the number of semantic classes. The semantic labeling of the image is converted to one-hot style and resized to $(w,h)$. This results in a mask $\mathbf{M}$ with shape $(w,h,s)$. Now multiplying $\mathbf{T}$ and $\mathbf{M}$ element-wise will result in each filter within $\mathbf{T}$ only focussing on one particular semantic class. Summing up $\mathbf{T}$ along the last dimension will result in a tensor of shape $(w,h,1)$ which then can be further processed as in the standard discriminator. 

\subsubsection{Network Architecture}
The images are resized to $256 \times 512$. The authors adapt the architecture from \cite{DBLP:journals/corr/IsolaZZE16} for the generator as well as the PatchGAN from the same work for the semantic-aware discriminator. 


\subsubsection{Training Details}
To stabilize training of their semantic aware discriminators $SD_R$ and $SD_V$, they use a history of refined images as proposed in \cite{DBLP:journals/corr/ShrivastavaPTSW16}. To generate higher quality images and stabilize training further the authors use least square objective for adversarial loss instead of log likelihood as proposed by \cite{DBLP:journals/corr/MaoLXLW16}. They set $\lambda_c = 10$, $\lambda_g = 5$ and $\alpha, \beta$ as $(1,0)$ for the first three epochs and change it to $(0.9, 0.1)$ thereafter in equation \ref{eq:final_obj_SG-GAN}. The sobel filters in equation \ref{eq:sobel} are used as gradient filters $\mathbf{C}_i$ in equation \ref{eq:soft-grad} and 
\begin{align}
		\begin{array}{cc}
		C_x = 
		\begin{pmatrix}
			0 & 0 & 0\\
			-1 & 0 & 1\\
			0 & 0 & 0
		\end{pmatrix},
		C_y = 
		\begin{pmatrix}
			0 & 1 & 0\\
			0 & 0 & 0\\
			0 & -1 & 0
		\end{pmatrix}
	\end{array}
\end{align}
for $\mathbf{C}_s$. In order to avoid sparse classes the authors cluster 30 \cite{Cordts_2016_CVPR} into 8 categories. Training is performed with a batch size of 1 and learning rate 0.0002. 


\paragraph{Unpaired data.} As all of these techniques use a cycle-consistency loss, it is possible to use unpaired datasets which makes data acquisition easier and reduces costs. This results in more training data and therefore makes better models possible. 