\chapter{Domain Adaptation Techniques}
\label{sec:techniques}

This chapter will give an overview over the three \textbf{Domain Adaptation Techniques} that will be compared in this work, namely \textbf{CycleGAN} \cite{DBLP:journals/corr/ZhuPIE17}, \textbf{CyCADA} \cite{DBLP:journals/corr/abs-1711-03213} and \textbf{SG-GAN} \cite{DBLP:journals/corr/abs-1801-01726}. It will explain how these techniques work, what kind of Network Architecture they use and how training these Nets was approached.

All of the methods compared in this work are based on Generative Adversarial Networks which were initially proposed in \cite{NIPS2014_5423} and are explained in Section \ref{sec:foundations}.\\
\section{CycleGAN} 
As proposed in \cite{DBLP:journals/corr/ZhuPIE17}, CycleGAN adds a cycle-consistency loss which consists of an additional generator/discriminator pair with the idea that translating a previously translated image back to the source domain should yield the original image again. Formally: let $G: X \rightarrow Y$ be the mapping function of the Generator translating an image from the source $X$ to the target domain $Y$. Now add mapping function $F: Y \rightarrow X$ for the second generator's translation from target back to the source domain. The goal of cycle-consistency is that for any given image $x\in X$ from the source domain, $F(G(x)) \approx x$ holds. This analogously also has to hold for any image $y \in Y$ of the target domain with $G(F(y)) \approx y$. As described in chapter \ref{sec:foundations}, GANs implement a two player game. The loss function of this game for CycleGAN is described as
\begin{align}
	\mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y\sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x\sim p_{\text{data}}(x)}[1 - \log D_Y(G(x))]
\end{align}
with $D_Y$ being the discriminator that classifies samples $y \in Y$ as real or fake. This loss function is analogous for generator $F$ and discriminator $D_X$. The cycle-consistency loss is defined as
\begin{align}
	\mathcal{L}_{\text{cyc}}(G, F) =  \mathbb{E}_{x\sim p_{\text{data}}(x)} [\lVert F(G(x))- x \rVert_1] + \mathbb{E}_{y\sim p_{\text{data}}(y)} [\lVert G(F(y))- y \rVert_1] \label{eq:GAN_loss}
\end{align}
with $\lVert \cdot \rVert_1$ being the L1-norm, which is the sum of absolute deviations between $x$ and $F(G(x))$ and $y$ and $G(F(y))$. The full objective for the CycleGAN method is therefore:
\begin{align}
	\mathcal{L}(G,F,D_X,D_Y) = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F) \label{eq:CycleGAN_loss}
\end{align}

with $\lambda$ controlling how much focus lies on the cycle-consistency. The discriminators' goal is to label the samples generated by the generators as fake while the generator tries to generate samples that the discriminator will label as real. This results in the goal of discriminators to maximize Equation \ref{eq:CycleGAN_loss} and generators to minimize it. This leads to the goal for CycleGAN: 
\begin{align}
	G^*, F^* = \arg \underset{G,F}{\min}\underset{D_X, D_Y}{\max} \mathcal{L}(G,F,D_X,D_Y) \label{eq:GAN_goal}
\end{align}
\section{CyCADA} 
CyCADA \cite{DBLP:journals/corr/abs-1711-03213} adds a semantic loss to the CycleGAN approach to enforce semantic consistency (no more translating cats to dogs or drawing trees into the sky). This is achieved by doing semantic segmentation on the source image, then translating that image and doing semantic segmentation on the target image. The loss describes the difference between the two resulting label maps. Formally they begin by learning a source model $f_X$ that does semantic segmentation on the source data. For K-way classification with cross-entropy loss this results in the following loss for the classification with $L_X$ being the source labels:
\begin{align}
	\mathcal{L}_{\text{task}}(f_X, X, L_X) = -\mathbb{E}_{(x, l_X)\sim(X, L_X)}\sum_{k=1}^{K}\mathds{1}_{[k=l_X]}\log\Bigl(\sigma(f_X^{(k)}(x))\Bigr) \label{eq:task_loss} 
\end{align}
where $\sigma$ denotes the softmax function.\\
With equation \ref{eq:task_loss} the semantic loss is defined as:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{sem}}(G, F, X, Y, f_X) &= \mathcal{L}_{\text{task}}(f_X, F(Y), p(f_X, Y))\\
		&+ \mathcal{L}_{\text{task}}(f_X, G(X), p(f_X, X))
	\end{split}
\end{align}
with $p(f_X, x)$ being the label predicted by source semantic segmentation model $f_X$ on a sample $x \in X$. Together with Equation \ref{eq:CycleGAN_loss} this results in the full objective of the CyCADA approach:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{CyCADA}}&(f_Y, X, Y, L_X, G, F, D_X, D_Y)\\
		&= \mathcal{L}_{\text{task}}(f_Y, G(X), L_X)\\
		&+ \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X)\\
		&+ \mathcal{L}_{\text{cyc}}(G, F, X, Y) + \mathcal{L}_{\text{sem}}(G, F, X, Y, f_X)
	\end{split}
\end{align}
And ultimately optimizing for a target model $f_Y$ with:
\begin{align}
	f^*_Y = \underset{f_Y}{\arg\min} ~ \underset{F,G}{\min} ~ \underset{D_X, D_Y}{\max} ~ \mathcal{L}_{\text{CyCADA}}&(f_X, X, Y, L_X, G, F, D_X, D_Y)
\end{align}
\section{SG-GAN}
Finally SG-GAN \cite{DBLP:journals/corr/abs-1801-01726} expands this by including a gradient-sensitive objective that emphasizes semantic boundaries by regularizing the generator to render distinct color/texture for each semantic region, and a patch-level discriminator that better understands differences in appearance distributions of different semantic regions (e.g. coarse texture of asphalt vs smooth and reflective of a vehicle). The soft gradient-sensitive objective is achieved by convolving gradient filters each upon an image and its semantic labeling. Typically Sobel filters are used:
\begin{align}
	\begin{array}{cc}
		C_x = 
		\begin{pmatrix}
			-1 & 0 & 1\\
			-2 & 0 & 2\\
			-1 & 0 & 1
		\end{pmatrix},
		C_y = 
		\begin{pmatrix}
			1 & 2 & 1\\
			0 & 0 & 0\\
			-1 & -2 & -1
		\end{pmatrix}
	\end{array}
\end{align}
These filters extract vertical (y-direction) and horizontal (x-direction) edges in an image. To get the gradient $C$ at each position, following formula is used:
\begin{align}
	C = \sqrt{C_x^2 + C_y^2}
\end{align}
With input image $v$ and corresponding semantic labeling $s_v$, the gradient sensitive loss can be defined as follows:
\begin{align}
		l_{\text{grad}}(v,s_v,G_{V\rightarrow R}) &= \lVert(|(|C_i * v |-|C_i*G_{V\rightarrow R}(v)|)|) \odot sgn(C_s*s_v)\rVert_1
\end{align}
where $C_i$ is the gradient filter for the image and $C_s$ the one for its semantic labeling map and $G_{V\rightarrow R}$ is the generator that maps from \underline{V}irtual to \underline{R}eal domain. $*$ describes the convolution of the filters over the image, $\odot$ is the element-wise multiplication, $|\cdot|$ is the absolute value- and $sgn$ the sign function. The loss grows, the higher the difference between gradients of image $v$ and image $G_{V\rightarrow R}(v)$. The element-wise multiplication with $sgn(C_s * s_v)$ enforces that the loss focuses on semantic boundaries only. For the soft gradient loss it may be believed that $v$ and $G_{V\rightarrow R}(v)$ share similar texture within semantic classes and therefore one will have edges where the other does aswell. Define following formula for the soft gradient-sensitive loss in which $\beta$ controls how much belief is set into the texture similarities:
\begin{align}
	\begin{split}
		l_{s-\text{grad}}(v, s_v, G_{V\rightarrow R}, \alpha, \beta) &= \lVert(|(|C_i*v|-|C_i*G_{V\rightarrow R}(v)|)|)\\
		&\odot (\alpha \times |sgn(C_s*s_v)|+\beta)\rVert_1\\
		&s.t. \quad \alpha + \beta = 1 \quad \alpha, \beta \geq 0
	\end{split} \label{eq:soft-grad}
\end{align}
With Equation \ref{eq:soft-grad} define the full soft gradient-sensitive loss with generator $G_{R\rightarrow V}$ from real to virtual and semantic labeling for real images $R$ as $S_R$ and $S_V$ for virtual images $V$:
\begin{align}
	\begin{split}
		\mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta) &= \mathbb{E}_{r\sim p_{\text{data}}(r)}[l_{s-\text{grad}}(r,s_r, G_{R\rightarrow V}, \alpha, \beta)]\\
		&+ \mathbb{E}_{v\sim p_{\text{data}}(v)}[l_{s-\text{grad}}(v,s_v, G_{V\rightarrow R}, \alpha, \beta)]
	\end{split}
\end{align}
The final objective with $\lambda_c, ~ \lambda_g$ controlling importance of cycle consistency- and soft gradient-sensitive loss relative to adversarial loss and semantic-aware Discriminators $SD_R,~SD_V$:
\begin{align}
	\begin{split}
		\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R) &= \mathcal{L}_{\text{GAN}}(G_{V\rightarrow R}, SD_R, V, R)\\
		&+ \mathcal{L}_{\text{GAN}}(G_{R\rightarrow V}, SD_V, R, V)\\
		&+ \lambda_c \mathcal{L}_{\text{cyc}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R)\\
		&+ \lambda_g \mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta)
	\end{split}
\end{align}
Which results in the optimization target:
\begin{align}
	G^*_{V\rightarrow R}, G^*_{R\rightarrow V} &= \arg \underset{\substack{G_{V\rightarrow R}\\ G_{R\rightarrow V}}}{\min}~ \underset{\substack{SD_R\\SD_V}}{\max}\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R)
\end{align}
The above mentioned semantic-aware discriminator enforces higher-level semantic consistencies. For example compared to the virtual world, real world may have less illumination. Instead of darkening the tone of the whole image it is instead more accurate to keep the sky bright and only darken e.g. the road. To achieve this semantic-aware judgement of realism, the discriminators' last layers' number of filters is transited to the number of semantic classes. Semantic masks are then applied upon these filters in order to make them focus on different semantic classes. Usually the last layer's feature map of the discriminator is a tensor $\mathbf{T}$ with shape $(w,h,1)$ with $w$ being the width and $h$ being the height. The SG-GAN approach changes this to $(w,h,s)$ where $s$ is the number of semantic classes. The semantic labeling of the image is converted to one-hot style and resized to $(w,h)$. This results in a mask $\mathbf{M}$ with shape $(w,h,s)$. Now multiplying $\mathbf{T}$ and $\mathbf{M}$ element-wise will result in each filter within $\mathbf{T}$ only focussing on one particular semantic class. Summing up $\mathbf{T}$ along the last dimension will result in a tensor of shape $(w,h,1)$ which then can be further processed as in the standard discriminator. As all of these techniques use a cycle-consistency loss, it is possible to use unpaired datasets which makes data acquisition easier and reduces costs. This results in more training data and therefore makes better models possible. 








%Prior to \cite{DBLP:journals/corr/ZhuPIE17} some works used paired images of the source and the target domain to train the networks. For example edgedrawings of shoes with a picture of the same shoe (see \cite{DBLP:journals/corr/IsolaZZE16}). The cycle-consistency loss of CycleGAN makes it possible to use datasets without paired images. This makes data acquisition easier and reduces costs which results in more training data and therefore makes better models possible. As CyCADA and SG-GAN are both based on CycleGAN they aswell don't require paired images.




%\section{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}
%see \cite{DBLP:journals/corr/ZhuPIE17}
%
%%\begin{itemize}
%%	\item image-to-image translation: extracting characteristics of an image and translating it to another style while preserving the characteristics (rgb to greyscale, painting to photo,..)
%%	\item special in this approach: no paired images necessary (datasets with paired images are far more expensive)
%%	\item create mapping $G: X \rightarrow Y$ from source domain $X$ to target domain $Y$
%%	\item The Generator has to trick the discriminator into believing $G(x), x \in X$ is actually a real sample $y$ from the target domain $Y$ (matches distribution $p_{\text{data}}(y)$)
%%	\item problem of mode collapse: any inputimage will be translated to the same output image
%%	\item add cycle-consistency constraint: create mapping $F: Y \rightarrow X$ and add contraint $F(G(x)) \overset{!}{\approx} x$ 
%%	\item objective for mapping/generator G and discriminator $D_Y$: \\
%%	$\mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y\sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x\sim p_{\text{data}}(x)}[1 - \log D_Y(G(x))]$
%%	\item analogous for mapping/generator F and discriminator $D_X$
%%	\item generators try to minimize the objective, discriminators try to maximize it
%%	\item cycle consistency loss:\\
%%	$\mathcal{L}_{\text{cyc}}(G, F) =  \mathbb{E}_{x\sim p_{\text{data}}(x)} [\lVert F(G(x))- x \rVert_1] + \mathbb{E}_{y\sim p_{\text{data}}(y)} [\lVert G(F(y))- y \rVert_1]$
%%	\item full objective:\\
%%	$\mathcal{L}(G,F,D_X,D_Y) = \mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) + \mathcal{L}_{\text{GAN}}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F)$
%%	\item solve: $G^*, F^* = \arg \underset{G,F}{\min}\underset{D_X, D_Y}{\max} \mathcal{L}(G,F,D_X,D_Y)$
%%\end{itemize}
%
%\subsection{Training Details}
%\begin{itemize}
%	\item for $\mathcal{L}_{\text{GAN}}$ replaced negative log-likelihood objective by least-squares loss $\rightarrow$ more stable during training and generates higher quality results
%	\item for GAN loss $\mathcal{L}_{\text{GAN}}(G, D, X, Y)$ they train \\
%	G to minimize $\mathbb{E}_{x\sim p_{\text{data}}(x)}[(D(G(x)) - 1)²]$ \\
%	and D to minimize $\mathbb{E}_{y\sim p_{\text{data}}(y)}[(D(y)-1)²] + \mathbb{E}_{x\sim p_{\text{data}}(x)}[D(G(x))²]$
%	\item reduce model oscillation \todo{add Shrivasta et al.}s method update discriminator using history f generated images instead of the latest ones generated. Use a buffer of 50 images
%	\item set $\lambda = 10$ in \todo{Equation 3}, Adam solver, batchsize 1, trained from scratch with learning rate 0.0002 for the first 100 epochs then decay linearly to 0 in the following 100. 
%\end{itemize}
%
%
%\newpage
%
%\section{CyCADA: Cycle Consistent Adversarial Domain Adaptation}
%
%see \cite{DBLP:journals/corr/abs-1711-03213}
%
%\subsection{Introduction}
%\begin{itemize}
%	\item synthetic datasets cheaper and more accurate in classification than real ones
%	\item per-pixel label accuracy drops from 93\%(real) to 54\%(synthetic)
%	\item while translating from synth to real semantic information might be lost (e.g translating line-drawing of a cat to a picture of a dog)
%	\item CyCADA uses cycle consistency and semantic losses
%	\item apply model to digit recognition and semantic segmentation of urban scenes across domains. 
%	\item improves per-pixel accuracy from 54\% to 82\% on synth-to-real. (\todo{compared to what?})
%	\item shows that domain adaptation benefits greatily from cycle-consistent pixel transformations
%	\item adaptation at both pixel and representation level can offer complementary improvements with joint pixel-space and feature adaptation leading to the highest performing model for digit classification tasks
%\end{itemize}
%
%\subsection{Related Work}
%
%\begin{itemize}
%	\item \todo{cite everything}
%	\item visual domain adaptation introduced along with a pairwise metric transform solution by Seanko et al. 2010
%	\item further popularized by broad study of visual dataset bias (Torralba \& Efros, 2011)
%	\item early deep adaptive works focused on feature space alignment through minimizing distance between first or second order feature space statistics of source and target (Tzeng et al., 2014; Long \& Wand, 2015)
%	\item further improved thorugh use of domain adversarial objectives whereby a domain classifier is trained to distinguish between source and target representations while domain representation is learned so as to maximize error of domain classifier
%	\item representation optimized by using standard minimax objective (Ganin \& Lempitsky, 2015)
%	\item symmetric confusion objective (Tzeng et al., 2015)
%	\item inverted label objective (Tzeng et al., 2017)
%	\item each related to GAN (Goodfellow et al., 2014) and followup trianing procedures for these networks (Salimans et al., 2016b; Arjovsky et al., 2017)
%	\item these feature-space adaptation methods focus on modifications to the discriminative representation space. Other recent methods hav sought adaptation in the pixel-space using various generative approaches
%	\item one advantage of pixel-space adaptation: result may be more human interpretable, since an image from on domain can now be visualized in a new domain
%	\item CoGANs (Liu \& Tuzel, 2016b) jointly learn source and target representation through explicit weight sharing of certain layers, source and target have unique gen adv objective
%	\item Ghifary et al. 2016 use an additional reconstruction objective in target domain to envourage alignment in the unsupervised adaptation setting
%	\item another approach: directly convert target image into a source style image (or vise versa), largely based on GANs (cite Goodfellow..)
%	\item successfully applied GANs to various applications such as image generation (Denton et al., 2015; Radford et al., 2015; Zhao et al., 2016), image editing (Zhu et al., 2016) and feature learning (Salimans et al., 2016a; Donahue et al., 2017). Recent work (Isola et al., 2016; Sangkloy et al., 2016; Karacan et al., 2016) adopt conditional GANs (Mirza \& Osindero, 2014) for these image-to-image translation problems (Isola et al., 2016), but require input-output image pairs for training, which is in general not available in domain adaptation problems
%	\item no training pairs: Yoo et al. 2016 learns source to target encoder-decoder along with a generative adversarial objective on reconstruction which is applied for predicting clothing people are wearing
%	\item Domain Transfer Network (Taigman et al. 2017b) trains generator to transform a source image into a target image by enforccing consistency in embedding space
%	\item Shrivastava et al. 2017 instead use L1 reconstruction loss to force generated target images to be similar to original source images. works well for limited domain shifts where domains are similar in pixel-space, but can be too limiting for setting with larger domain shifts
%	\item Bousmalis et al. 2017b use a content similarity loss to ensure the generated target image is similar to original source image; however this requires prior knowledge about which parts of the image stay the same across domains (e.g. foreground)
%	\item cycada method does not require pre-defining what content is shared between domains and instead simply translates images back to their original domains while ensuring that the remain identical to their original version
%	\item BiGAN (Donahue et al., 2017) and ALI (Dumoulin et al., 2016) take an approach of simultaneously learning the transofmrations between pixel and latent space.
%	\item CycleGAN (Zhu et al., 2017) produced compelling image translation results such as generating photorealistic images from impressionism paintings or transfomring horses into zebras at high resolution using cycle-consistency loss
%	\item this loss was simultaneously proposed by Yi et al. 2017 and Kim et al. 2017 to great effect as well
%	\item adaptation across weather conditions in simple road scenes was first studied by Levinkov \& and Fritz 2013
%	\item convolutional domain adversarial based approach was proposed for more general drive cam scenes and for adaptation from simulated to real environments (Hoffmann et al., 2016)
%	\item Ros et al. 2016b learns a multi-source model through concatenating all available labeled data and learning a single large model and then transfers to a sparsely labeled target domain through distillation (Hinton et al., 2015)
%	\item Chen et al. 2017 use an adversarial objective to align both global and class-specific statistics, while mining additional temporal data from street view datasets to learn static object prior
%	\item Zhang et al. 2017 instead perform segmentation adaptation by aligning label distributions both globally and across superpixels in an image
%\end{itemize}
%
%%\subsection{Cycle-consistent adversarial domain adaptation}
%%\begin{itemize}
%%	\item consider problem of unsupervised adaptation
%%	\item provided source data $X_S$, source labels $Y_S$, and target data $X_T$, but no target labels
%%	\item goal: learn a model $f$ that can correctly predict label for target data $X_T$
%%	\item begin by learning source model $f_S$ that can perform the task on the source data
%%	\item for K-way classification with cross-entropy loss:
%%	\begin{align}
%%	\mathcal{L}_{\text{task}}(f_S, X_S, Y_S) = -\mathbb{E}_{(x_s, y_s)\sim(X_S, Y_S)}\sum_{k=1}^{K}\mathds{1}_{[k=y_s]}\log\Bigl(\sigma(f_S^{(k)}(x_s))\Bigr)
%%	\end{align}
%%	where $\sigma$ denotes softmax function
%%	\item learned model $f_S$ will perform well on source data but typically domain shift between source and target domain leads to reduced performance when evaluating on target data
%%	\item by mapping samples into common space, the model can learn on source data while still generalizing to target data
%%	\item mapping from source to target $G_{S\rightarrow T}$ trained to produce target samples that fool adversarial discriminator $D_T$
%%	\item discriminator attempts to classify real target data from source target data. loss function:
%%	\begin{align}
%%		\mathcal{L}_{\text{GAN}}(G_{S\rightarrow T}, D_T, X_T, X_S) &= \mathbb{E}_{x_t \sim X_T}\Bigl[\log D_T(x_t)\Bigr] \\
%%		&+ \mathbb{E}_{x_s \sim X_S} \Bigl[\log(1-D_T(G_{S\rightarrow T}(x_s)))\Bigr]
%%	\end{align}
%%	\item this objective ensures that $G_{S\rightarrow T}$, given source samples, produces convincing target samples
%%	\item this ability to directly map samples between domains allows to learn target model $f_T$ by minimizing $\mathcal{L}_{\text{task}}(f_T, G_{S\rightarrow T}(X_S), Y_S)$
%%	\item previous approaches that optimized similar objectives have shown effetive results but in practice can often be unstable and prone to failure
%%	\item although GAN loss ensures $G_{S\rightarrow T}(x_s)$ for some $x_s$ will resemble data drawn from $X_T$ but there is no way to guarantee $G_{S\rightarrow T}(x_s)$ preserves structure or content of original sample $x_s$
%%	\item to encourage source content to be preserved during conversion: cycle-consistency constraint (\todo{cite the 3 works that proposed it}). Mapping $G_{T \rightarrow S}$ trained according to GAN loss $\mathcal{L}_{\text{GAN}}(G_{T \rightarrow S}, D_S, X_S, X_T)$
%%	\item want $G_{T \rightarrow S}(G_{S \rightarrow T}(x_s)) \approx x_s$ and $G_{S \rightarrow T}(G_{T \rightarrow S}(x_t)) \approx x_t$
%%	\item done by imposing L1 penalty on reconstruction error (reffered to as cycle-consistency loss):
%%	\begin{align}
%%		\mathcal{L}_{\text{cyc}}(G_{S\rightarrow T}, G_{T\rightarrow S}, X_S, X_T) &= \mathbb{E}_{x_s \sim X_S} \Bigl[\lVert G_{T\rightarrow S}(G_{S\rightarrow T}(x_s)) - x_s\rVert_1\Bigr]\\
%%		&+ \mathbb{E}_{x_t \sim X_T} \Bigl[\lVert G_{S\rightarrow T}(G_{T\rightarrow S}(x_t)) - x_t\rVert_1\Bigr]
%%	\end{align}
%%	\item also explicitly encourage high semantic consistency before and after image translation
%%	\item pretrain source task model $f_S$, fixing weights and using it as a noisy labeler by which an image to be classified in the same way after translation as it was before translation according to this classifier is encouraged
%%	\item define fixed classifier f, predicted label for given input $X$: $p(f,X) = \arg \max(f(X))$
%%	\item semantic consistency before and after image translation:
%%	\begin{align}
%%		\mathcal{L}_{\text{sem}}(G_{S\rightarrow T}, G_{T\rightarrow S}, X_S, X_T, f_S) &= \mathcal{L}_{\text{task}}(f_S, G_{T\rightarrow S}(X_T), p(f_S, X_T))\\
%%		&+ \mathcal{L}_{\text{task}}(f_S, G_{S\rightarrow T}(X_S), p(f_S, X_S))
%%	\end{align}
%%	\item can be viewed analogously to content loss in style transfer (Gatys et al., 2016) or in pixel adaptation (Taigman et al., 2017a), where shared content to preserve is dictated by the source task model $f_S$
%%	\item could also consider a feature-level method which discriminates between the features or semantics from two image sets as viewed under a task network $\rightarrow$ additional feature level GAN loss:
%%	\begin{align}
%%		\mathcal{L}_{\text{GAN}}(f_T, D_{\text{feat}}, f_S(G_{S\rightarrow T}(X_S)), X_T)
%%	\end{align}
%%	\item together form complete objective:
%%	\begin{align}
%%		\mathcal{L}_{\text{CyCADA}}&(f_T, X_S, X_T, Y_S, G_{S\rightarrow T}, G_{T\rightarrow S}, D_S, D_T)\\
%%		&= \mathcal{L}_{\text{task}}(f_T, G_{S\rightarrow T}(X_S), Y_S)\\
%%		&+ \mathcal{L}_{\text{GAN}}(G_{S\rightarrow T}, D_T, X_T, X_S) + \mathcal{L}_{\text{GAN}}(G_{T\rightarrow S}, D_S, X_S, X_T)\\
%%		&+ \mathcal{L}_{\text{GAN}}(f_T, D_{\text{feat}}, f_S(G_{S\rightarrow T}(X_S)), X_T)\\
%%		&+ \mathcal{L}_{\text{cyc}}(G_{S\rightarrow T}, G_{T\rightarrow S}, X_S, X_T) + \mathcal{L}_{\text{sem}}(G_{S\rightarrow T}, G_{T\rightarrow S}, X_S, X_T, f_S)
%%	\end{align}
%%	\item ultimately corresponds to solving for a target model $f_T$ according to the optimization problem
%%	\begin{align}
%%		f^*_T = \underset{f_T}{\arg\min} ~ \underset{\substack{G_{T\rightarrow S} \\ G_{S\rightarrow T}}}{\min} ~ \underset{D_S, D_T}{\max} ~ \mathcal{L}_{\text{CyCADA}}&(f_T, X_S, X_T, Y_S, G_{S\rightarrow T}, G_{T\rightarrow S}, D_S, D_T)
%%	\end{align}
%%\end{itemize}
%
%\newpage
%
%\section{Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption}
%
%see \cite{DBLP:journals/corr/abs-1801-01726}
%
%\subsection{Introduction}
%\begin{itemize}
%	\item two main contributions: 
%	\begin{enumerate}
%		\item gradient-sensitive objective, emphasizes the semantic boundary consistencies between virtual images and adapted images. able to regularize the generator render distinct color/texture for each semantic region in order to keep semantic boundaries, which can alleviate the common blurry issues
%		\item previous work often learn a whole image discriminator, makes pixels in image easily collapse into a monotonous pattern. appearance distributions for each semantic region should be regarded differently and purposely (e.g. road: coarse texture of asphalt concrete, vehicle: smooth and reflective)
%	\end{enumerate}
%	\item semantic-aware discriminator learns distinct discriminate parameters for examining regions with respect to each semantic label
%	\item SG-GAN controllable architecture that personalizes texture rendering for different semantic regions and results in adapted images with finer details
%\end{itemize}
%
%\subsection{Related Work}
%\subsubsection{Real-world vs. virtual-world data acquiring}
%\begin{itemize}
%	\item -
%\end{itemize}
%\subsubsection{Domain adaptation}
%\begin{itemize}
%	\item either by adapting scene images or adapting hidden feature representations guided by the targets
%	\item Image-based adaption (aka image-to-image translation) can be summarized into two directions:
%	\begin{enumerate}
%		\item generated through feature matching (e.g. \todo{cite Gatys et al. 10}: combine content of one with style of other images through matching Gram matrix on deep feature maps, at expense of loss of some content information)
%		\item GAN (e.g. \todo{Isola et al. 21} conditional GANs with mapping function for paired data, for unpaired data e.g. regularization term \todo{cite following} 37, cycle structure 24 45 47, weight sharing 29 30)
%	\end{enumerate}
%	\item some make use of both feature matching and adversarial training (5, 44)
%	\item challenge: existing approaches often modify semantic information, e.g. sky adapted to tree structure or road lamp rendered from nothing
%	\item hidden feature based adaption aims at adapting learned models to target domain (\todo{multiple citations}), by sharing weight (12) or incorporating adversarial discriminative setting (39), those mitigate performance degradation caused by domain shifting
%	\item feature based adaption methods require different objectives or architecture for different vision tasks, thus not as widely applicable as image-based adaption
%\end{itemize}
%
%\subsection{Semantic-aware Grad-GAN}
%
%\subsubsection{Semantic-aware cycle objective}
%\begin{itemize}
%	\item based on cycle-structured GAN objective
%	\item unpaired images from virtual-world domain $V$ and real-world domain $R$ as $\{v\}^N_{i=1} \in V$ and $\{r\}^M_{i=1} \in R$
%	\item SG-GAN learns symmetric mappings $G_{V\rightarrow R}$ and $G_{R\Rightarrow V}$ alogn with corresponding semantic-aware discriminators $SD_R, SD_V$
%	\item $SD_R$ distinguishes between real-world images $\{r\}$ and fake real-world images $\{G_{V\rightarrow R}(v)\}$ and vice versa for $SD_V$
%\end{itemize}
%
%\subsubsection{Adversarial loss}
%
%\begin{itemize}
%	\item two sets of adversial losses applied to $(G_{V\Rightarrow R}, SD_R)$ and $(G_{R\Rightarrow V}, SD_V)$ pairs
%	\item adversarial loss:
%	\begin{align}
%	\mathcal{L}_{\text{adv}}(G_{V\rightarrow R}, SD_R, V, R) &= \mathbb{E}_{r\sim p_{\text{data}}(r)}[\log SD_R(r)]\\
%	&+ \mathbb{E}_{v\sim p_{\text{data}}(v)}[\log(1-SD_R(G_{V\rightarrow R}(v)))]
%	\end{align}
%	\item objective
%	\begin{align}
%	G^*_{V\rightarrow R} = \arg \underset{G_{V\rightarrow R}}{\min} \underset{SD_R}{\max} \mathcal{L}_{\text{adv}}(G_{V\rightarrow R}, SD_R, V, R)
%	\end{align}
%	\item analogous for other generator discriminator: $\mathcal{L}_{\text{adv}}(G_{R\rightarrow V}, SD_V, R, V)$
%\end{itemize}
%
%\subsubsection{Cycle consistency loss}
%\begin{align}
%\mathcal{L}_{\text{cyc}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R) &= \mathbb{E}_{r\sim p_{\text{data}}(r)}[\lVert G_{V\rightarrow R}(G_{R\rightarrow V}(r)) - r \rVert_1]\\
%&+ \mathbb{E}_{v\sim p_{\text{data}}(v)}[\lVert G_{R\rightarrow V}(G_{V\rightarrow R}(v)) - v \rVert_1]
%\end{align}
%
%\begin{itemize}
%	\item can be seen as introduction of a regularization on positions of image elements
%	\item moving positions of image components is not encouraged
%	\item model only with cycle-consistency can still map sky to tree (not semantic aware)
%\end{itemize}
%
%\subsubsection{Soft gradient-sensitive objective}
%
%\begin{itemize}
%	\item motivation of gradient-sensitive loss: no matter how texture of semantic classes change, there should be some distinguishable visual difference at boundaries of semantic classes
%	\item visual differences for adjacent pixels can be captured through convolving gradient filters upon the image
%	\item typical choice of gradient filter is Sobel filter (\todo{cite 38}) as $\mathbf{C} = \{C_x, C_y\}$:
%	$\begin{array}{cc}
%	C_x = 
%	\begin{pmatrix}
%	-1 & 0 & 1\\
%	-2 & 0 & 2\\
%	-1 & 0 & 1
%	\end{pmatrix},
%	C_y = 
%	\begin{pmatrix}
%	1 & 2 & 1\\
%	0 & 0 & 0\\
%	-1 & -2 & -1
%	\end{pmatrix}
%	\end{array}$
%	\item since focus is visual difference on semantic boundaries, a 0-1 mask is necessary that only has non-zero values on semantic boundaries
%	\item such mask can be retrieved by convolving a gradient filter upon semantic labeling since it only has different adjacent values on semantic boundaries
%	\item semantic labeling obtained by human annotation, segmentation models, computer graphics tools (\todo{cite})
%	\item multiply convolved semantic labeling and convolved image element-wise, to only pay attention to visual differences on semantic boundaries
%	\item for input image $v$ and corresponding semantic labeling $s_v$ since we desire $v$ and $G_{V\rightarrow R}(v)$ share the same semantic information, greadient-sensitive loss for image $v$ can be defined as follows with $C_i$ and $C_s$ gradient filters for image and semantic labeling, $*$ convolution, $\odot$ element-wise multiplication, $|\cdot|$ absolute value,
%	$\lVert \cdot \rVert_1$ L1-norm, $sgn$ sign function:
%	\begin{align}
%	l_{\text{grad}}(v,s_v,(G_{V\rightarrow R})) &= \lVert(|(|C_i * v |-|C_i*G_{V\rightarrow R}(v)|)|)\\
%	&\odot sgn(C_s*s_v)\rVert_1
%	\end{align}
%	\item in practice we may hold belief that $v$ and $G_{V\rightarrow R}(v)$ share smiliar texture within semantic classes
%	\item texture can be extracted from image gradient, therefore a soft gradient-sensitive loss for $v$ can be defined as following to represent such belief, in which $\beta$ controls how much belief we have on texture similarities
%	\begin{align}
%	l_{s-\text{grad}}(v, s_v, G_{V\rightarrow R}, \alpha, \beta) &= \lVert(|(|C_i*v|-|C_i*G_{V\rightarrow R}(v)|)|)\\
%	&\odot (\alpha \times |sgn(C_s*s_v)|+\beta)\rVert_1\\
%	&s.t. \quad \alpha + \beta = 1 \quad \alpha, \beta \geq 0
%	\end{align}
%	\item $S_V$ semantic labeling for $V$, $S_R$ for R. final objective for soft gradient-sensitive loss for single image:
%	\begin{align}
%	\mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta) &= \mathbb{E}_{r\sim p_{\text{data}}(r)}[l_{s-\text{grad}}(r,s_r, G_{R\rightarrow V}, \alpha, \beta)]\\
%	&+ \mathbb{E}_{v\sim p_{\text{data}}(v)}[l_{s-\text{grad}}(v,s_v, G_{V\rightarrow R}, \alpha, \beta)]
%	\end{align}
%	\item full objective with $\lambda_c$, $\lambda_g$ controlling relative importance of cycle consistency and soft gradient-sensitive loss compared to adversarial loss:
%	\begin{align}
%	\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R) &= \mathcal{L}_{\text{adv}}(G_{V\rightarrow R}, SD_R, V, R)\\
%	&+ \mathcal{L}_{\text{adv}}(G_{R\rightarrow V}, SD_V, R, V)\\
%	&+ \lambda_c \mathcal{L}_{\text{cyc}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R)\\
%	&+ \lambda_g \mathcal{L}_{\text{grad}}(G_{V\rightarrow R}, G_{R\rightarrow V}, V, R, S_V, S_R, \alpha, \beta)
%	\end{align}
%	\item optimization target:
%	\begin{align}
%	G^*_{V\rightarrow R}, G^*_{R\rightarrow V} &= \arg \underset{\substack{G_{V\rightarrow R}\\ G_{R\rightarrow V}}}{\min}~ \underset{\substack{SD_R\\SD_V}}{\max}\mathcal{L}(G_{V\rightarrow R}, G_{R\rightarrow V}, SD_V, SD_R)
%	\end{align}
%\end{itemize}
%
%\subsection{Semantic-aware discriminator}
%\begin{itemize}
%	\item introduction of soft gradient-sensitive loss contributes to smoother textures and clearer semantic boundaries
%	\item scene adaption also needs to retain higher-level semantic consistencies (e.g. tone goes dark cause real world less ilumination, but may only want roads to be darker without changing the sky or even making it lighter)
%	\item inappropriate holistic scene adaption because of traditional discriminator judging realism image-wise, regardless of texture difference in semantic-aware manner
%	\item semantic-aware discriminators $SD_V$, $SD_R$
%	\item idea: create separate pipeline for each different semantic class in the discriminator
%	\item can be achieved by transiting number of filters in last layer of standard discriminator to number of semantic classes, then applying semantic masks upon filter to let each of them focus on different semantic classes
%	\item last ($k$-th) layer's feature map of standard discriminator is typically a tensor $\mathbf{T}_k$ with shape $(w_k, h_k, 1)$, where $w_k$ stands for width and $h_k$ stands for height
%	\item $T_k$ then compared with an all-one or all-zero tensor to calculate adversarial objective
%	\item in contrast semantic-aware discriminator will change $\mathbf{T}_k$ as a tensor with shape $(w_k, h_k,s)$ where $s$ is number of semantic classes
%	\item then convert image's semantic labeling to one-hot style and resize to $(w_k, h_k)$ which will result in mask $M$ with shape $(w_k, h_k, s)$ and $\mathbf{M}_{ij} \in \{01\}$
%	\item by multiplying $\mathbf{T}_k$ and $\mathbf{M}$ element-wise, each filter within $\mathbf{T}_k$ will only focus on one particular semantic class
%	\item Finally, by summing up $\mathbf{T}_k$ along the last dimension, tensor with shape $(w_k, h_k, 1)$ will be acquired and adversarial objective can be calculated the same way as standard discriminator
%\end{itemize}
