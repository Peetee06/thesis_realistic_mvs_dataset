\chapter{Introduction}
\label{sec:introduction}
With autonomous driving being highly popular these days and the need for large amounts of labeled data to train the deep neural networks running the autonomous cars, methods have been developed to generate that data. Real datasets like Cityscapes \cite{Cordts_2016_CVPR} or Berkley Deep Drive \cite{DBLP:journals/corr/abs-1805-04687} are expensive to assemble and even more expensive to label. Especially pixel-level annotations require a lot of working hours and can be inaccurate. In contrast synthetic approaches like the GTA5 \cite{Richter_2016_ECCV} or SYNTHIA \cite{RosCVPR16} datasets enable to create vast amounts of image data automatically and make the annotation process much faster and more accurate and therefore cheaper than their real counterparts. The drawback of using synthetic data to train models that are ran in autonomous cars is that these models don't perform well in real environments. This can result in wrong predictions, which must be avoided in order to make autonomous driving possible. The drop in performance stems from the domain shift, i.e. changes in appearance, texture and lighting of objects in synthetic images compared to objects in the real world seen through the car's cameras. An approach to create models to perform better in this scenario is domain adaptation. The idea behind domain adaptation is to adapt a specific source domain to another target domain (e.g. synthetic to real, image taken at night to image taken at daytime, images of a specific artistic style to another artistic style, ...). For autonomous driving this means making the synthetic image look more like an image of a real scene taken with a real camera. There are many techniques for adapting images from a synthetic to a realistic domain. Each having different approaches and using different methods. This work compares three current approaches on synthetic-to-real domain adaptation, namely CycleGAN \cite{DBLP:journals/corr/ZhuPIE17}, CyCADA \cite{DBLP:journals/corr/abs-1711-03213} and SG-GAN \cite{DBLP:journals/corr/abs-1801-01726}, all three of which are based on Generative Adversarial Networks as first proposed in \cite{NIPS2014_5423}. And tries to show their strengths and weaknesses when using them to translate images from the GTA5 dataset, a large scale synthetic dataset of images from a street view in the game Grand Theft Auto V \cite{Richter_2016_ECCV}, to images looking like the ones in Cityscapes, a large scale dataset of real street view images of european cities \cite{Cordts_2016_CVPR}. In order to evaluate the resulting images, a DeepLabv3 \cite{DBLP:journals/corr/ChenPSA17} model pre-trained on the Cityscapes dataset (code repository: \cite{DLR}) is used to predict semantic segmentation maps. The Cityscapes evaluation code \cite{CSR} then yields Intersection over Union (IoU) results for these, comparing predicted pixels with ground truth pixels. 