\chapter{Introduction}

With autonomous driving being highly popular these days and the need for large amounts of labeled data to train the deep neural networks running the autonomous cars, methods have been developed to generate that data. Real datasets like Cityscapes \cite{Cordts_2016_CVPR} or the Berkley Deep Drive \cite{DBLP:journals/corr/abs-1805-04687} are expensive to assemble and even more expensive to label. Especially pixel-level annotations require a lot of working hours and can be inaccurate. In contrast synthetic approaches like the GTAV \cite{Richter_2016_ECCV} or SYNTHIA datasets \cite{RosCVPR16} enable to create vast amounts of image data automatically and make the annotation process much faster and more accurate and therefore cheaper than their real counterparts. The drawback of using synthetic data to train models that are ran in autonomous cars is that these models don't perform well in real environments. This can result in wrong predictions, which must be avoided in order to make autonomous driving possible. The drop in performance is due to the domain shift, i.e. changes in appearance, texture and others of objects in the synthetic images compared to objects in the real world seen through the car's cameras. An approach to create models to perform better in this scenario is domain adaptation. The idea behind that is to adapt a specific domain to another domain (e.g. synthetic to real, image taken at night to image taken at daytime, images of an object taken from one perspective to another perspective, ...). For autonomous driving this means making the synthetic image look more like an image of a real scene taken with a real camera. There are many techniques for adapting images from a synthetic to a realistic domain. Each having different approaches and using different methods. This work compares three current approaches on synthetic-to-real domain adaptation, namely CycleGAN \cite{DBLP:journals/corr/ZhuPIE17}, CyCADA \cite{DBLP:journals/corr/abs-1711-03213} and SG-GAN/Grad-GAN \cite{DBLP:journals/corr/abs-1801-01726}, all three of which are based on Generative Adversarial Networks \cite{NIPS2014_5423}. And tries to show their strengths and weaknesses when using them to translate images from the GTAV dataset \cite{Richter_2016_ECCV} to images looking like the ones in Cityscapes \cite{Cordts_2016_CVPR}. In order to evaluate the resulting images, a pre-trained deeplabv3 \cite{DBLP:journals/corr/ChenPSA17} model (code: \cite{DLR}) is used to predict semantic segmentation maps and use the cityscapes evaluation code \cite{CSR} to compute Intersection over Union (IoU) for these. 