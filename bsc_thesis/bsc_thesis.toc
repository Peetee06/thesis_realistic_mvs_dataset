\contentsline {chapter}{\numberline {1}Introduction}{13}% 
\contentsline {section}{\numberline {1.1}Problem Statement}{13}% 
\contentsline {chapter}{\numberline {2}Foundations}{15}% 
\contentsline {section}{\numberline {2.1}Domain Adaptation}{15}% 
\contentsline {section}{\numberline {2.2}Neural Networks}{16}% 
\contentsline {subsection}{\numberline {2.2.1}Convolutional Neural Networks}{16}% 
\contentsline {paragraph}{\nonumberline Convolutional Layer.}{16}% 
\contentsline {paragraph}{\nonumberline Pooling Layer}{16}% 
\contentsline {paragraph}{\nonumberline Fully Connected Layer}{16}% 
\contentsline {subsection}{\numberline {2.2.2}Generative Adversarial Networks}{16}% 
\contentsline {paragraph}{\nonumberline Generators}{17}% 
\contentsline {paragraph}{\nonumberline Discriminators}{17}% 
\contentsline {paragraph}{\nonumberline Training}{17}% 
\contentsline {paragraph}{\nonumberline Advantages and Disandvantages}{17}% 
\contentsline {chapter}{\numberline {3}Related Work}{19}% 
\contentsline {section}{\numberline {3.1}Domain Adaptation for Structured Output via Discriminative Patch Representation}{19}% 
\contentsline {subsection}{\numberline {3.1.1}Abstract}{19}% 
\contentsline {subsection}{\numberline {3.1.2}Introduction}{19}% 
\contentsline {subsection}{\numberline {3.1.3}domain adaptation for structured output}{20}% 
\contentsline {section}{\numberline {3.2}Effective Use of Synthetic Data for Urban Scene Semantic Segmentation}{22}% 
\contentsline {subsection}{\numberline {3.2.1}related work}{23}% 
\contentsline {subsection}{\numberline {3.2.2}Method}{23}% 
\contentsline {subsection}{\numberline {3.2.3}Implementation Details}{24}% 
\contentsline {section}{\numberline {3.3}Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency}{24}% 
\contentsline {subsection}{\numberline {3.3.1}Method}{25}% 
\contentsline {subsection}{\numberline {3.3.2}Learning}{26}% 
\contentsline {subsection}{\numberline {3.3.3}Experiments}{26}% 
\contentsline {subsection}{\numberline {3.3.4}Discussion}{27}% 
\contentsline {section}{\numberline {3.4}Exploiting Semantics in Adversarial Training for Image-Level Domain Adaptation}{27}% 
\contentsline {section}{\numberline {3.5}FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation}{27}% 
\contentsline {subsection}{\numberline {3.5.1}work includes}{27}% 
\contentsline {subsection}{\numberline {3.5.2}Related Work}{28}% 
\contentsline {subsubsection}{\nonumberline Domain Adaptation}{29}% 
\contentsline {subsection}{\numberline {3.5.3}Fully Convolutional Adaptation Models}{29}% 
\contentsline {subsection}{\numberline {3.5.4}Global Domain Alignment}{31}% 
\contentsline {subsection}{\numberline {3.5.5}Category Specific Adaptation}{32}% 
\contentsline {subsection}{\numberline {3.5.6}Experiments}{33}% 
\contentsline {subsection}{\numberline {3.5.7}Datasets}{34}% 
\contentsline {subsubsection}{\nonumberline Cityscapes}{34}% 
\contentsline {subsubsection}{\nonumberline SYNTHIA}{34}% 
\contentsline {subsubsection}{\nonumberline GTA5}{34}% 
\contentsline {subsubsection}{\nonumberline BDDS}{34}% 
\contentsline {subsection}{\numberline {3.5.8}Quantitative and Qualitative Results}{35}% 
\contentsline {subsubsection}{\nonumberline Large Shift: Synthetic to Real Adaptation}{35}% 
\contentsline {subsubsection}{\nonumberline Medium Shift: Cross Seasons Adaptation}{35}% 
\contentsline {subsubsection}{\nonumberline Small Shift: Cross City Adaptation}{35}% 
\contentsline {subsection}{\numberline {3.5.9}BDDS Adaptation}{35}% 
\contentsline {section}{\numberline {3.6}From Virtual to Real World Visual Perception using Domain Adaptation - The DPM Example}{36}% 
\contentsline {subsection}{\numberline {3.6.1}Need for Virtual Worlds}{36}% 
\contentsline {subsection}{\numberline {3.6.2}Need for Domain Adaptation}{36}% 
\contentsline {subsection}{\numberline {3.6.3}Domain Adaptation for DPM in a Nutshell}{36}% 
\contentsline {section}{\numberline {3.7}Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption}{37}% 
\contentsline {subsection}{\numberline {3.7.1}Introduction}{37}% 
\contentsline {subsection}{\numberline {3.7.2}Related Work}{37}% 
\contentsline {subsubsection}{\nonumberline Real-world vs. virtual-world data acquiring}{37}% 
\contentsline {subsubsection}{\nonumberline Domain adaptation}{37}% 
\contentsline {subsection}{\numberline {3.7.3}Semantic-aware Grad-GAN}{38}% 
\contentsline {subsubsection}{\nonumberline Semantic-aware cycle objective}{38}% 
\contentsline {subsubsection}{\nonumberline Adversarial loss}{38}% 
\contentsline {subsubsection}{\nonumberline Cycle consistency loss}{39}% 
\contentsline {subsubsection}{\nonumberline Soft gradient-sensitive objective}{39}% 
\contentsline {subsection}{\numberline {3.7.4}Semantic-aware discriminator}{40}% 
\contentsline {subsection}{\numberline {3.7.5}Experiments}{41}% 
\contentsline {subsection}{\numberline {3.7.6}Experiments - Implementation}{41}% 
\contentsline {subsubsection}{\nonumberline Dataset}{41}% 
\contentsline {subsubsection}{\nonumberline Network architecture}{41}% 
\contentsline {subsubsection}{\nonumberline Training details}{42}% 
\contentsline {subsubsection}{\nonumberline Testing}{42}% 
\contentsline {subsection}{\numberline {3.7.7}Comparison with state-of-the-art methods}{43}% 
\contentsline {subsection}{\numberline {3.7.8}Comparison - Baselines}{43}% 
\contentsline {subsubsection}{\nonumberline SimGAN}{43}% 
\contentsline {subsubsection}{\nonumberline CycleGAN}{43}% 
\contentsline {subsubsection}{\nonumberline DualGAN}{43}% 
\contentsline {subsubsection}{\nonumberline BiGAN}{43}% 
\contentsline {subsection}{\numberline {3.7.9}Comparison - Gualitative and quantitative evaluation}{43}% 
\contentsline {subsection}{\numberline {3.7.10}Ablation studies}{44}% 
\contentsline {subsubsection}{\nonumberline Effectiveness of soft gradient-sensitive objective}{44}% 
\contentsline {subsubsection}{\nonumberline .. of semantic-aware discriminator}{44}% 
\contentsline {subsubsection}{\nonumberline The effect of virtual training image size}{44}% 
\contentsline {subsubsection}{\nonumberline Discussion}{44}% 
\contentsline {subsection}{\numberline {3.7.11}Application on semantic segmentation}{45}% 
\contentsline {section}{\numberline {3.8}VisDA: The Visual Domain Adaptation Challenge}{45}% 
\contentsline {subsection}{\numberline {3.8.1}Introduction}{45}% 
\contentsline {subsection}{\numberline {3.8.2}Related Work}{46}% 
\contentsline {subsubsection}{\nonumberline Classification Datasets}{46}% 
\contentsline {subsubsection}{\nonumberline Semantic Segmentation Datasets}{47}% 
\contentsline {subsubsection}{\nonumberline Synthetic Datasets}{47}% 
\contentsline {subsection}{\numberline {3.8.3}VisDA-C: Classification Dataset}{47}% 
\contentsline {subsection}{\numberline {3.8.4}Dataset Acquisition}{48}% 
\contentsline {subsubsection}{\nonumberline Training Domain: CAD-Synthetic Images}{48}% 
\contentsline {subsubsection}{\nonumberline Validation Domain: MS COCO}{48}% 
\contentsline {subsubsection}{\nonumberline Testing Domain: YouTube Bounding Boxes}{49}% 
\contentsline {subsection}{\numberline {3.8.5}Experiments}{49}% 
\contentsline {subsubsection}{\nonumberline Experimental Setup}{49}% 
\contentsline {subsubsection}{\nonumberline Domain Adaptation Algorithms}{50}% 
\contentsline {subsubsection}{\nonumberline Baseline results}{50}% 
\contentsline {subsection}{\numberline {3.8.6}Increasing Difficulty and Future Research}{51}% 
\contentsline {subsection}{\numberline {3.8.7}VisDA-S: Semantic Segmentation}{52}% 
\contentsline {subsubsection}{\nonumberline Training Domain: Synthetic GTA5}{52}% 
\contentsline {subsubsection}{\nonumberline Validation Domain: Real CityScapes}{52}% 
\contentsline {subsubsection}{\nonumberline Test Domain: Real DashCam Images}{52}% 
\contentsline {subsubsection}{\nonumberline Domain Adaptation Algorithms}{52}% 
\contentsline {subsubsection}{\nonumberline Baseline Results}{53}% 
\contentsline {subsubsection}{\nonumberline Challenge Results}{53}% 
\contentsline {chapter}{\numberline {4}Domain Adaptation Techniques}{55}% 
\contentsline {section}{\numberline {4.1}Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}{55}% 
\contentsline {subsection}{\numberline {4.1.1}Training Details}{56}% 
\contentsline {section}{\numberline {4.2}CyCADA: Cycle Consistent Adversarial Domain Adaptation}{57}% 
\contentsline {subsection}{\numberline {4.2.1}Introduction}{57}% 
\contentsline {subsection}{\numberline {4.2.2}Related Work}{57}% 
\contentsline {subsection}{\numberline {4.2.3}Cycle-consistent adversarial domain adaptation}{59}% 
\contentsline {section}{\numberline {4.3}Technique 3}{62}% 
\contentsline {chapter}{\numberline {5}Experiments}{63}% 
\contentsline {section}{\numberline {5.1}training the nets on tcml cluster}{63}% 
\contentsline {section}{\numberline {5.2}Datasets}{63}% 
\contentsline {subsection}{\numberline {5.2.1}Synthetic dataset: \\ Playing for Data: Ground Truth from Computer Games}{63}% 
\contentsline {subsection}{\numberline {5.2.2}Real dataset:\\ The Cityscapes Dataset for Semantic Urban Scene Understanding}{64}% 
\contentsline {section}{\numberline {5.3}comparison Benchmark(s)}{65}% 
\contentsline {subsection}{\numberline {5.3.1}Intersection over Union (IoU)}{65}% 
\contentsline {subsection}{\numberline {5.3.2}Perceptual Loss}{65}% 
\contentsline {chapter}{\numberline {6}Conclusion}{67}% 
\contentsline {chapter}{\numberline {A}Blub}{69}% 
